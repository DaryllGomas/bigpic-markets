# Cloud AI & Inference Platforms

*Research Date: February 2026*

---

## 1. Cloud AI Market Overview

| Metric | Value | Source/Note |
|--------|-------|-------------|
| Global cloud AI market (2025) | ~$90-122B | Varies by analyst; MarketsAndMarkets: $89B, Grand View: $122B |
| Projected 2026 | ~$170B | Grand View Research |
| Projected 2030 | ~$363B | MarketsAndMarkets (32.4% CAGR) |
| Overall cloud computing market (2025) | ~$943B | Approaching $1T in 2026 |
| AI PaaS sub-segment CAGR | 51.1% (5yr) | IDC — fastest PaaS growth category |
| IaaS market (2024) | $171.8B | Grew 22.5% YoY |
| Global cloud infra spend Q3 2025 | $102.6B/quarter | 25% YoY growth |

### Hyperscaler Capex — The Arms Race

| Year | Combined Hyperscaler Capex (Big 5) | YoY Growth |
|------|-------------------------------------|------------|
| 2024 | ~$256B | +63% |
| 2025E | ~$443B | +73% |
| 2026E | ~$600B+ | +36% |

Roughly 75% (~$450B in 2026) is directly tied to AI infrastructure (servers, GPUs, data centers).

---

## 2. Hyperscaler AI Revenue

### Microsoft Azure

| Metric | Value |
|--------|-------|
| Azure annual revenue | $75B+ (FY2025) |
| Azure YoY growth | 34-39% (AI-driven acceleration) |
| Azure AI annual run rate | ~$26B (2025), up from $13B earlier in FY2025 |
| AI contribution to Azure growth | ~19 percentage points of Azure growth |
| Enterprise GenAI hosting share | 42% of enterprises use Azure as primary GenAI platform |
| Cloud market share | 22% |
| Key advantage | OpenAI partnership — priority access to GPT models |

Azure AI grew from $13B run rate to $26B within a single fiscal year — 175%+ growth. Microsoft is on track for $10B in annual revenue from AI inference alone.

### Amazon Web Services (AWS)

| Metric | Value |
|--------|-------|
| AWS quarterly revenue (Q3 2025) | $33B+ |
| AWS YoY growth | 17-20% |
| Cloud market share | 29-32% (market leader) |
| Trainium custom chip status | Multi-billion-dollar business, 150% QoQ growth |
| Bedrock token usage | Majority already running on Trainium |
| Capacity added in 2025 | 5GW |

AWS Bedrock + Trainium strategy: positioning Trainium3 as lead inference engine, with stated ambition for Bedrock to become "as big as EC2." Announced Trainium3 at re:Invent 2025 with 40% better price-performance than Trainium2.

### Google Cloud Platform (GCP)

| Metric | Value |
|--------|-------|
| Google Cloud annual revenue | $33B+ |
| GCP YoY growth | 32-36% |
| Cloud market share | 11-12% |
| Vertex AI usage growth | 20x YoY |
| Gemini developer count | 4M+ developers building with Gemini |
| TPU generation | 7th gen (Ironwood) — 5x compute, 6x HBM vs prior gen |

Google's differentiation: proprietary TPU silicon + Gemini model family. Vertex AI seeing strongest platform adoption growth among the three.

---

## 3. CoreWeave (CRWV)

### Company Profile

| Metric | Value |
|--------|-------|
| Ticker | CRWV (Nasdaq) |
| IPO Date | March 2025 |
| IPO Price | $40/share |
| IPO Raise | $1.5B (originally targeted $2.7B — 44% haircut) |
| Initial Market Cap | ~$35B |
| GPU Fleet | 250,000+ NVIDIA GPUs |
| Data Centers | 33 facilities |
| Active Power | 470MW |

### Revenue Trajectory

| Period | Revenue |
|--------|---------|
| 2022 | $16M |
| 2024 | $1.92B (737% YoY growth) |
| Q3 2025 | $1.4B (single quarter, +134% YoY) |
| 2025 Guidance | $5.05-5.15B |
| 2026 Analyst Estimate | ~$11.6B |

### Key Contracts

| Customer | Deal Value | Duration |
|----------|-----------|----------|
| OpenAI | $6.5B expansion | Multi-year |
| Meta | Up to $14.2B | 6 years |
| Microsoft | Was 62% of 2024 revenue | Ongoing |

Revenue backlog: $55.6B with 2.9GW contracted power.

### Bear Case

- **Customer concentration:** Microsoft was 62% of 2024 revenue (diversifying with OpenAI/Meta deals)
- **GAAP net loss:** $863M in 2024 despite revenue growth
- **Massive debt load:** Aggressive GPU financing creates leverage risk
- **GPU depreciation:** 6-year depreciation schedule may overstate useful life if Blackwell offers 4-5x inference speedup over H100
- **IPO priced below expectations:** Market skepticism on profitability path
- **Guidance miss:** Q4 2025 data center delivery delays pushed supply to Q1 2026

---

## 4. GPU Cloud Startups (Neoclouds)

### Lambda Labs

| Metric | Value |
|--------|-------|
| Latest funding | $1.5B Series E (Nov 2025, led by TWG Global) |
| Total raised | $2.44B over 10 rounds |
| Nvidia deal | $1.5B GPU leaseback (18,000 GPUs over 4 years) |
| Microsoft deal | Multi-billion, multi-year — tens of thousands of GPUs |
| IPO target | Originally H1 2026, likely pushed to late 2026+ |
| Differentiation | "Superintelligence Cloud" — liquid-cooled US data centers |

### Together AI

| Metric | Value |
|--------|-------|
| Latest funding | $305M Series B (Feb 2025) |
| Total raised | $534M over 4 rounds |
| Valuation | $3.3B |
| Lead investors | General Catalyst, Prosperity7 |
| Differentiation | Open-source model cloud — train, fine-tune, and run frontier open models |
| Notable | Self-service GPU infrastructure launched Sep 2025, Blackwell GPU deployment |

### Crusoe Energy

| Metric | Value |
|--------|-------|
| Latest funding | $1.38B Series E (Oct 2025) |
| Valuation | $10B+ |
| Revenue (2024) | $276M |
| Revenue (2025E) | ~$1B |
| Revenue (2026E) | ~$2B |
| Flagship facility | 1.2GW campus in Abilene, TX (400,000 GB200 GPUs) |
| Financing | $11.6B for Abilene campus |
| Energy cost advantage | 30-50% lower than traditional hyperscalers |

Note: Crusoe sold its Digital Flare Mitigation and Bitcoin mining division to NYDIG in March 2025. Now focused purely on AI data centers with natural gas + renewable power.

### Voltage Park

| Metric | Value |
|--------|-------|
| Structure | Nonprofit-backed (Navigation Fund, Jed McCaleb) |
| Initial grant | $500M |
| GPU fleet | 24,000+ GPUs across 6+ data centers |
| GPU types | H100, B200, GB300 |
| Acquisitions | TensorDock (Mar 2025) |
| Partnerships | VAST Data (AI Operating System deployment) |
| Differentiation | Nonprofit model enables below-market pricing |

---

## 5. Inference-Optimized Silicon

### Groq

| Metric | Value |
|--------|-------|
| Technology | LPU (Language Processing Unit) — ASIC for inference |
| Series D (2024) | $640M at $2.8B valuation |
| 2025 Raise | $750M at $6.9B valuation |
| Nvidia acquisition | ~$20B deal announced Dec 2025 |
| Revenue projection (2025) | $500M |
| Saudi deal | $1.5B commitment for LPU inference infrastructure |
| Performance | 1,345 tok/s (Llama-3 8B), 662 tok/s (Qwen-3 32B) |
| Speed advantage | ~5x faster than standard GPU inference; 7.5x vs Blackwell (claimed) |

Groq's acquisition by Nvidia is the marquee M&A event — validates inference-specific silicon as strategic.

### Cerebras

| Metric | Value |
|--------|-------|
| Technology | WSE-3 (Wafer-Scale Engine) — entire 300mm wafer as single processor |
| Specs | 4 trillion transistors, 900,000 AI cores |
| Latest funding | $1B (Feb 2026) |
| Valuation | $23B |
| Revenue (2025E) | $1B+ |
| IPO timeline | Q2 2026 (Nasdaq) |
| Customers | IBM, US Dept of Energy |
| Performance claim | Up to 21x faster than equivalent NVIDIA clusters (Llama-4) |
| Prior blocker | CFIUS review over G42/UAE investor ties — resolved via investor restructuring |

### SambaNova

| Metric | Value |
|--------|-------|
| Technology | Reconfigurable Dataflow Architecture (RDU) — enterprise inference |
| Latest funding | $350M Series E (Feb 2026, led by Vista Equity + Intel) |
| Intel relationship | CEO Lip-Bu Tan is SambaNova exec chairman; Intel acquisition talks at $1.6B stalled |
| Key contracts | 4 data center operators including sovereign cloud in Scotland (2GW+ campus) |
| Positioning | Enterprise-focused inference platform |

### d-Matrix

| Metric | Value |
|--------|-------|
| Technology | Digital In-Memory Compute (DIMC) for data center inference |
| Series C | $275M (Nov 2025) |
| Valuation | $2B |
| Total raised | $450M |
| Focus | Large-scale data center inference deployments |

### Comparison

| Company | Approach | Valuation | Key Advantage |
|---------|----------|-----------|---------------|
| Groq | LPU ASIC | ~$20B (Nvidia deal) | Raw inference speed leader |
| Cerebras | Wafer-scale | $23B | Massive parallelism, training + inference |
| SambaNova | RDU (dataflow) | ~$2B+ | Enterprise focus, Intel alignment |
| d-Matrix | In-memory compute | $2B | Low-power inference at scale |

---

## 6. Training vs. Inference Shift

### Spending Split Over Time

| Year | Training | Inference |
|------|----------|-----------|
| 2023 | ~67% | ~33% |
| 2024 | ~60% | ~40% |
| 2025 | ~50% | ~50% |
| 2026E | ~45% | ~55% |
| 2030E | ~20-25% | ~75-80% |

2026 is the crossover year — inference spending exceeds training for the first time, reaching 55% of AI cloud infrastructure spend ($20.6B in absolute terms for cloud infra alone).

### Inference Market Size

| Metric | Value |
|--------|-------|
| AI inference market (2025) | $106B |
| Projected 2030 | $255B (19.2% CAGR) |
| Inference chip market (2026E) | $50B+ |

### Cost Per Token Trends

The most dramatic deflation in tech history:

| Benchmark | Date | Cost/M tokens | Decline |
|-----------|------|---------------|---------|
| GPT-3.5 level | Nov 2022 | $20.00 | — |
| GPT-3.5 level | Oct 2024 | $0.07 | 280x cheaper |
| GPT-4 output | Mar 2023 | $60.00 | — |
| GPT-4o output | Late 2024 | $10.00 | 6x cheaper |
| GPT-4 equivalent | 2025 | $0.40 | 150x vs original |

Post-Jan 2024 acceleration: median price decline of 200x per year (up from 50x/year prior). DeepSeek disrupted market with 90% lower pricing than incumbents.

### Training Cost Trends

| Metric | Value |
|--------|-------|
| GPT-4 training cost | ~$40M |
| Gemini Ultra training cost | ~$30M |
| Cost growth rate | 2.4x per year since 2016 |
| Cost breakdown | 47-67% hardware, 29-49% staff (incl equity), 2-6% energy |
| Projected: largest runs by 2027 | $1B+ |

---

## 7. GPU Cloud Economics

### Pricing Trends ($/GPU-hour, on-demand)

| GPU | Neocloud Low | Hyperscaler Range | Trend |
|-----|-------------|-------------------|-------|
| H100 | $2.10 | $4.00-$8.00 | Median $2.99; heading sub-$2 by mid-2026 |
| H200 | $2.50 | $3.72-$10.60 | 40-80% better perf than H100 for memory tasks |
| B200 | Quote-based | Quote-based | Limited availability, pricing still forming |

### Unit Economics

| Metric | Value | Note |
|--------|-------|------|
| Target utilization | 80%+ | Required for healthy returns |
| Typical utilization (mature) | 50-60% | With take-or-pay contracts |
| Early deployment utilization | ~30% | First year as customers onboard |
| Gross margin (before depreciation) | 55-65% | |
| Gross margin (after labor, power, depreciation) | 14-16% | Thin margin of safety |
| Mature EBITDA margin (Year 3+) | 30-35% | At scale with optimized costs |

### Depreciation Risk

GPU depreciation is the hidden risk in neocloud economics:
- CoreWeave depreciates over **6 years**
- New NVIDIA architectures ship every **1-2 years**
- Blackwell offers **4-5x** faster inference than H100
- Counter-argument: GPUs "cascade" from training to inference to less demanding tasks, extending economic life
- If true useful life is 3-4 years vs. 6-year depreciation, near-term profits are overstated

### Contract Structures

- **Take-or-pay:** Customer commits to minimum spend regardless of usage — provides revenue visibility
- **Reserved instances:** 1-3 year commitments at 40-60% discount vs on-demand
- **Spot/preemptible:** Volatile pricing, ~50-70% discount, can be interrupted
- CoreWeave revenue backlog model: $55.6B backlog provides multi-year visibility

---

## 8. Edge AI / On-Device Inference

### Market Size

| Metric | Value |
|--------|-------|
| On-device AI market (2025) | $10.8B |
| Projected 2033 | $75.5B (27.8% CAGR) |
| Edge AI hardware market (2025) | $26-28B |
| Projected 2031 | $68.7B (17.5% CAGR) |
| AI PC market share (2025) | 31% of total PCs |
| AI PC market share (2026E) | 54.7% of total PCs |
| AI PC shipments (2025) | 77.8M units |
| AI PC shipments (2026E) | 143.1M units |

### Key Players

| Company | Product | NPU Performance | Notes |
|---------|---------|-----------------|-------|
| Apple | Neural Engine (A18 Pro) | 35 TOPS | 20% less power; iPhones, iPads, Macs, Vision Pro |
| Qualcomm | Snapdragon X Elite | 45 TOPS NPU | Premium AI laptops |
| Intel | Core Ultra 300 | 50 TOPS NPU | Can run 13B parameter models locally |
| AMD | Ryzen AI | 40+ TOPS | Competing in AI PC segment |
| MediaTek | Dimensity 9400 | 40+ TOPS | Mobile SoC leader in mid-range |

Smartphones contributed 39.25% of edge AI hardware revenue in 2025.

---

## 9. AI PaaS / Model Hosting Platforms

### Hugging Face

| Metric | Value |
|--------|-------|
| Revenue (2024) | ~$130M |
| Revenue (2023) | ~$70M (367% growth from 2022) |
| Valuation | $4.5B |
| Total raised | ~$396M |
| Users | 5M+ |
| Models hosted | 1M+ |
| Customers | 50,000+ |
| Revenue model | Free tier + Pro ($9/mo) + Team ($20/mo) + Enterprise (managed deployments) |

De facto hub for open-source AI models. Enterprise consulting contracts (Nvidia, Amazon, Microsoft) drive majority of revenue.

### Fireworks AI

| Metric | Value |
|--------|-------|
| ARR (May 2025) | $130-280M (sources vary) |
| Growth | ~20x YoY |
| Series C (Oct 2025) | $254M ($230M primary + $24M secondary) |
| Valuation | $4B |
| Total raised | $331M |
| Lead investors | Lightspeed, Index, Nvidia, Sequoia |
| Positioning | Enterprise AI inference platform — fastest-growing in category |

### Replicate

| Metric | Value |
|--------|-------|
| Revenue (2024) | ~$5.3M |
| Total raised | $57.8M |
| Status | Acquired by Cloudflare (Nov 2025) |
| Model | Pay-as-you-go GPU markup + developer tooling |

Cloudflare acquisition signals consolidation — inference hosting moving into CDN/edge networks.

### Anyscale (Ray)

| Metric | Value |
|--------|-------|
| Valuation | $1B (Series C) |
| Total raised | $160M |
| Estimated ARR (2026) | $100M+ |
| Key milestone | Ray transferred to PyTorch Foundation (Oct 2025) — now neutral standard |
| Azure partnership | Private preview Nov 2025, GA expected 2026 |
| Key customers | OpenAI, Uber, Canva |

### Modal

Limited public financial data. Positioned as serverless GPU cloud for developers. Competes with RunPod, Replicate (now Cloudflare), and Lambda for developer-focused inference workloads.

---

## 10. Sovereign AI Cloud

### Major National Initiatives

| Country/Region | Initiative | Scale |
|----------------|-----------|-------|
| **EU** | Cloud and AI Development Act (2026 requirements) | EUR 180M tender for sovereign cloud infra |
| **EU** | AWS European Sovereign Cloud | EUR 7.8B investment, Germany launch late 2025 |
| **EU** | Microsoft Sovereign Private Cloud | Air-gapped France/Germany deployments |
| **EU** | Google Sovereign Cloud Hub | Munich launch Nov 2025 |
| **Saudi Arabia** | HUMAIN + AWS partnership | 150,000 AI accelerators incl GB300; 18,000 GPU DC in 2026 |
| **Saudi Arabia** | Groq LPU deal | $1.5B commitment for inference infrastructure |
| **India** | Sovereign LLM launch | AI Impact Summit Feb 2026; Bhashini platform (22 languages) |
| **Japan** | KDDI + Google Cloud | Regional sovereign cloud partner |
| **Scotland** | SambaNova sovereign cloud | 2GW+ campus capacity |

### Market Size

Global sovereign cloud market projected to exceed $250B within 3 years. Key drivers:
- Data residency regulations (GDPR, national security)
- AI sovereignty concerns (model training on national data)
- Strategic independence from US hyperscalers
- Gulf states pivoting from oil to AI compute

---

## 11. Investment Implications

### Bull Themes

1. **Inference is the next mega-cycle:** Crossing 50% of AI compute spend in 2026, heading to 75-80% by 2030. Every deployed AI application needs continuous inference — it's the "electricity meter" of AI.

2. **Hyperscaler capex is self-reinforcing:** $600B+ in 2026 capex means massive demand for GPUs, networking, power, and cooling. AWS, Azure, GCP all reporting AI revenue acceleration and demand exceeding supply.

3. **Neocloud validation:** CoreWeave's $55B backlog, Lambda's Nvidia/Microsoft deals, Crusoe's $11.6B campus — enterprise and hyperscaler demand for specialized GPU cloud is real and growing.

4. **Inference silicon is strategic:** Nvidia's $20B Groq acquisition + Cerebras's $23B valuation confirm that inference-optimized chips are critical infrastructure, not niche.

5. **Edge AI creates new TAM:** AI PCs going from 31% to 55% of shipments in one year. 143M AI PCs in 2026. On-device inference is a new silicon demand vector.

### Bear Themes

1. **Token price deflation is brutal:** 200x/year price declines mean inference revenue per query is collapsing. Revenue growth requires volume to massively outpace price compression.

2. **Neocloud leverage risk:** CoreWeave's $863M net loss, aggressive depreciation assumptions, customer concentration. GPU cloud is capital-intensive with thin margins (14-16% net of depreciation).

3. **Hyperscaler capex may overshoot:** $600B in 2026 — if AI adoption slows or reasoning/efficiency breakthroughs reduce compute needs, overcapacity risk is real.

4. **Depreciation time bomb:** 6-year depreciation on GPUs that face 4-5x performance jumps every 2 years. If utilization of older GPUs falls faster than expected, write-downs could be significant.

5. **DeepSeek/open-source disruption:** Efficient open models (DeepSeek, Llama) reduce the amount of compute needed per task, potentially slowing infrastructure demand growth.

6. **Sovereign cloud fragmentation:** Balkanization of AI compute could reduce economies of scale and increase costs across the ecosystem.

### Key Metrics to Watch

- Inference-to-training spending ratio (crossing 60% would confirm the shift)
- CoreWeave utilization rates and GAAP profitability timeline
- H100/H200 spot pricing (sub-$2/hr would signal oversupply)
- Hyperscaler AI revenue disclosures (Azure AI run rate, AWS Trainium revenue)
- Groq integration into Nvidia ecosystem
- Cerebras IPO reception (Q2 2026)
- Token price vs. volume metrics at API providers
- Edge AI attach rates in enterprise PC refresh cycles

---

*Data sourced from company earnings reports, SEC filings, industry analyst reports (MarketsAndMarkets, Grand View Research, Mordor Intelligence, IDC), and financial news coverage. All figures as of February 2026 unless otherwise noted.*
